Metadata-Version: 2.4
Name: openvla
Version: 0.0.3
Summary: OpenVLA: Vision-Language-Action Models for Robotics
Author-email: Moo Jin Kim <moojink@stanford.edu>, Karl Pertsch <pertsch@berkeley.edu>, Siddharth Karamcheti <skaramcheti@cs.stanford.edu>
License: MIT License
        
        Copyright (c) 2024 Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti.
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: homepage, https://github.com/openvla/openvla
Project-URL: repository, https://github.com/openvla/openvla
Project-URL: documentation, https://github.com/openvla/openvla
Keywords: vision-language-actions models,multimodal pretraining,robot learning
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: accelerate>=0.25.0
Requires-Dist: draccus==0.8.0
Requires-Dist: einops
Requires-Dist: huggingface_hub
Requires-Dist: json-numpy
Requires-Dist: jsonlines
Requires-Dist: matplotlib
Requires-Dist: peft==0.11.1
Requires-Dist: protobuf
Requires-Dist: rich
Requires-Dist: sentencepiece==0.1.99
Requires-Dist: timm==0.9.10
Requires-Dist: tokenizers==0.19.1
Requires-Dist: torch==2.2.0
Requires-Dist: torchvision==0.17.0
Requires-Dist: torchaudio==2.2.0
Requires-Dist: transformers==4.40.1
Requires-Dist: wandb
Requires-Dist: tensorflow==2.15.0
Requires-Dist: tensorflow_datasets==4.9.3
Requires-Dist: tensorflow_graphics==2021.12.3
Requires-Dist: dlimp@ git+https://github.com/moojink/dlimp_openvla
Provides-Extra: dev
Requires-Dist: black>=24.2.0; extra == "dev"
Requires-Dist: gpustat; extra == "dev"
Requires-Dist: ipython; extra == "dev"
Requires-Dist: pre-commit; extra == "dev"
Requires-Dist: ruff>=0.2.2; extra == "dev"
Provides-Extra: sagemaker
Requires-Dist: boto3; extra == "sagemaker"
Requires-Dist: sagemaker; extra == "sagemaker"
Dynamic: license-file

# OpenVLA: An Open-Source Vision-Language-Action Model

[**官方文档**](https://openvla.github.io/) | [**复现环境**](#复现环境) | [**LIBERO**](#LIBERO)


<hr style="border: 2px solid gray;"></hr>

## 官方文档

[![Homepage](https://img.shields.io/badge/Homepage-blue?style=for-the-badge)](https://openvla.github.io/)
[![arXiv](https://img.shields.io/badge/arXiv-2406.09246-df2a2a.svg?style=for-the-badge)](https://arxiv.org/abs/2406.09246)
[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-Models-yellow?style=for-the-badge)](https://huggingface.co/openvla/openvla-7b)
[![License](https://img.shields.io/github/license/TRI-ML/prismatic-vlms?style=for-the-badge)](LICENSE)

```bibtex
@article{kim24openvla,
    title={OpenVLA: An Open-Source Vision-Language-Action Model},
    author={{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},
    journal = {arXiv preprint arXiv:2406.09246},
    year={2024}
} 
```

## 复现环境配置

复现结果使用以下设备及环境：

GPU: NVIDIA Tesla A100

硬件架构：| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |

[![Python](https://img.shields.io/badge/python-3.10-blue?style=for-the-badge)](https://www.python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.0-EE4C2C.svg?style=for-the-badge&logo=pytorch)](https://pytorch.org/get-started/locally/)

```bash
conda create -n openvla python=3.10 -y
conda activate openvla

# 原论文指出该项目要满足Python 3.10.13、PyTorch 2.2.0、transformers 4.40.1 和 flash-attn 2.5.5的环境
# 原版的环境，CUDA 12.4
# conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y  
# CUDA 12.1
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y
# CUDA 11.8
# conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=11.8 -c pytorch -c nvidia -y

git clone https://github.com/ZYangChen/openvla.git
cd openvla
cd dlimp_openvla
pip install -e .
# pip uninstall sympy -y #若有环境冲突，可尝试该过程
# pip install sympy==1.13.1
cd ..
pip install -e . --no-deps
pip install -r requirements-vla.txt

# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)
#   =>> If you run into difficulty, try `pip cache remove flash_attn` first
pip install packaging ninja
ninja --version; echo $?  # Verify Ninja --> should return exit code "0"
pip install "flash-attn==2.5.5" --no-build-isolation
```

<hr style="border: 2px solid gray;"></hr>

## LIBERO

### 环境

在已有环境上进一步配置

```bash
pip install -r experiments/robot/libero/libero_requirements.txt #在openvla目录下
git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git
cd LIBERO
pip install -e .

apt-get install libegl1 mesa-utils libgl1-mesa-glx
```

也可以考虑手动下载[LIBERO库](https://github.com/Lifelong-Robot-Learning/LIBERO)。

### 数据集

下载地址：https://libero-project.github.io/datasets

### 权重

```bash
pip install -U huggingface_hub

export HF_ENDPOINT=https://hf-mirror.com

# 下载fine-tuned OpenVLA via LoRA (r=32) on four LIBERO task suites independently: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-10 (also called LIBERO-Long).
huggingface-cli download --resume-download openvla/openvla-7b-finetuned-libero-spatial --local-dir ./weight/libero/openvla-7b-finetuned-libero-spatial

```

The four checkpoints are available on Hugging Face:
* [openvla/openvla-7b-finetuned-libero-spatial](https://huggingface.co/openvla/openvla-7b-finetuned-libero-spatial)
* [openvla/openvla-7b-finetuned-libero-object](https://huggingface.co/openvla/openvla-7b-finetuned-libero-object)
* [openvla/openvla-7b-finetuned-libero-goal](https://huggingface.co/openvla/openvla-7b-finetuned-libero-goal)
* [openvla/openvla-7b-finetuned-libero-10](https://huggingface.co/openvla/openvla-7b-finetuned-libero-10)

### 推理

#### e.g. LIBERO-Spatial

数据集路径可以在推理时设置，也可以直接修改config.yaml

```bash
# Launch LIBERO-Spatial evals
#CUDA_VISIBLE_DEVICES=1 MUJOCO_EGL_DEVICE_ID=1 
python experiments/robot/libero/run_libero_eval.py \
  --model_family openvla \
  --pretrained_checkpoint weight/libero/openvla-7b-finetuned-libero-spatial \
  --task_suite_name libero_spatial \
  --use_wandb False \
  --center_crop True

# Do you want to specify a custom path for the dataset folder? (Y/N):
Y

# Please enter the path to the dataset folder:
/remote-home/path_to_your_root/vla/openvla/datasets # 你的数据集路径 <PATH TO DATASET FOLDER>
```

或者

```bash
vim /root/.libero/config.yaml

datasets: /remote-home/path_to_your_root/vla/openvla/datasets
```

| Method | LIBERO-Spatial | LIBERO-Object | LIBERO-Goal | LIBERO-Long | Average |
|--------|----------------|---------------|-------------|-------------|---------|
| Diffusion Policy from scratch | 78.3 ± 1.1% | **92.5 ± 0.7%** | 68.3 ± 1.2% | 50.5 ± 1.3% | 72.4 ± 0.7% |
| Octo fine-tuned | 78.9 ± 1.0% | 85.7 ± 0.9% | **84.6 ± 0.9%** | 51.1 ± 1.3% | 75.1 ± 0.6% |
| OpenVLA fine-tuned | **84.7 ± 0.9%** | 88.4 ± 0.8% | 79.2 ± 1.0% | **53.7 ± 1.3%** | **76.5 ± 0.6%** |

Each success rate is the average over 3 random seeds x 500 rollouts each (10 tasks x 50 rollouts per task).

更新中...


